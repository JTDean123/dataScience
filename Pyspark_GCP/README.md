# PySpark_GCP
building a linear regression model with PySpark on the Google Cloud Platform   

The goal of this project is predict the tip amount paid for a a taxi ride in NYC in 2015 with a linear regression model. The main technical challenge in this project is the size of the data set: 77106102 observations! This is obviously too much data to simply download locally and load into memory via a pandas dataframe, so in this project I focus less on the typical model building techniques like data cleaning and parameter tuning and more on exploring the technologies needed to manipulate and explore such a large data set.   

For this project I created an Apache Spark cluster consisting of 1 master node and 4 worker nodes on the Google Cloud Plaform (GCP). Apache Spark is an open source platform for fast analytics on large datasets and fortunately a Python API (PySpark) exists. The core principal of Spark is the notion of Resilient Distributed Datasets (RDD). In the Spark framework a dataset is divided into RDDs and distributed to different compute nodes on a cluster, allowing us to essentially partition and apply operations on a dataset over a cluster size of our choice.  I found that data frame operations like counting and aggregating for the purpose of calculating summary statistics completed in less than five minutes, demonstrating that spark and cluster computing an awesome tool for wrestling large data sets like this one.   
